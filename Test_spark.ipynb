{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/usr/local/spark\")\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup spark session\n",
    "**Prior Spark 2.0, Spark Context was the entry point of any spark application\n",
    "and used to access all spark features and needed a sparkConf \n",
    "which had all the cluster configs and parameters to create a Spark Context object. \n",
    "We could primarily create just RDDs using Spark Context \n",
    "and we had to create specific spark contexts for any other spark interactions. \n",
    "For SQL SQLContext, hive HiveContext, streaming Streaming Application. \n",
    "In a nutshell, Spark session is a combination of all these different contexts. \n",
    "Internally, Spark session creates a new SparkContext for all the operations \n",
    "and also all the above-mentioned contexts can be accessed using the SparkSession object.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.0-preview2\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"SparkTest\").master(\"local[4]\").getOrCreate()\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD \n",
    "represents **Resilient Distributed Dataset**. An RDD in Spark is simply an immutable distributed\n",
    "collection of objects sets. Each RDD is split into multiple partitions (similar pattern with smaller sets),\n",
    "which may be computed on different nodes of the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create RDD \n",
    " - **From csv file** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+---------+-----+\n",
      "|_c0|   TV|Radio|Newspaper|Sales|\n",
      "+---+-----+-----+---------+-----+\n",
      "|  1|230.1| 37.8|     69.2| 22.1|\n",
      "|  2| 44.5| 39.3|     45.1| 10.4|\n",
      "|  3| 17.2| 45.9|     69.3|  9.3|\n",
      "|  4|151.5| 41.3|     58.5| 18.5|\n",
      "|  5|180.8| 10.8|     58.4| 12.9|\n",
      "|  6|  8.7| 48.9|       75|  7.2|\n",
      "|  7| 57.5| 32.8|     23.5| 11.8|\n",
      "|  8|120.2| 19.6|     11.6| 13.2|\n",
      "|  9|  8.6|  2.1|        1|  4.8|\n",
      "| 10|199.8|  2.6|     21.2| 10.6|\n",
      "| 11| 66.1|  5.8|     24.2|  8.6|\n",
      "| 12|214.7|   24|        4| 17.4|\n",
      "| 13| 23.8| 35.1|     65.9|  9.2|\n",
      "| 14| 97.5|  7.6|      7.2|  9.7|\n",
      "| 15|204.1| 32.9|       46|   19|\n",
      "| 16|195.4| 47.7|     52.9| 22.4|\n",
      "| 17| 67.8| 36.6|      114| 12.5|\n",
      "| 18|281.4| 39.6|     55.8| 24.4|\n",
      "| 19| 69.2| 20.5|     18.3| 11.3|\n",
      "| 20|147.3| 23.9|     19.1| 14.6|\n",
      "| 21|218.4| 27.7|     53.4|   18|\n",
      "| 22|237.4|  5.1|     23.5| 12.5|\n",
      "| 23| 13.2| 15.9|     49.6|  5.6|\n",
      "| 24|228.3| 16.9|     26.2| 15.5|\n",
      "| 25| 62.3| 12.6|     18.3|  9.7|\n",
      "+---+-----+-----+---------+-----+\n",
      "only showing top 25 rows\n",
      "\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- TV: string (nullable = true)\n",
      " |-- Radio: string (nullable = true)\n",
      " |-- Newspaper: string (nullable = true)\n",
      " |-- Sales: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").load(\"data_csv/Adv.csv\", header=True)\n",
    "df.show(25)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **using parallelize() of sparkContext**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+-----+\n",
      "|col1|col2|col3| col4|\n",
      "+----+----+----+-----+\n",
      "|   1|   2|   3|a b c|\n",
      "|   4|   5|   6|d e f|\n",
      "|   7|   8|   9|g h i|\n",
      "+----+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2 = spark.sparkContext.parallelize([(1, 2, 3, 'a b c'),(4, 5, 6, 'd e f'),\n",
    "                          (7, 8, 9, 'g h i')]).toDF(['col1', 'col2', 'col3','col4'])\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **using createDataFrame()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+------------+\n",
      "| Id| Name|Sallary|DepartmentId|\n",
      "+---+-----+-------+------------+\n",
      "|  1|  Joe|  70000|           1|\n",
      "|  2|Henry|  80000|           2|\n",
      "|  3|  Sam|  60000|           2|\n",
      "|  4|  Max|  90000|           1|\n",
      "+---+-----+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Employee = spark.createDataFrame(\n",
    "    [('1', 'Joe', '70000', '1'),\n",
    "    ('2', 'Henry', '80000', '2'),\n",
    "    ('3', 'Sam', '60000', '2'),\n",
    "    ('4', 'Max', '90000', '1')],\n",
    "    ['Id', 'Name', 'Sallary','DepartmentId']\n",
    ")\n",
    "Employee.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDDs support two types of operations:\n",
    "#### transformations (https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations), which create a new dataset from an existing one, \n",
    "#### and actions (https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions), which return a value to the driver program after running a computation on the dataset.**\n",
    "\n",
    "<img src=\"img/t.png\"/>\n",
    "<img src=\"img/a1.png\"/>\n",
    "<img src=\"img/a2.png\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'string'),\n",
       " ('TV', 'string'),\n",
       " ('Radio', 'string'),\n",
       " ('Newspaper', 'string'),\n",
       " ('Sales', 'string')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0', 'Radio', 'Newspaper']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_name = ['TV', 'Sales']\n",
    "df.drop(*drop_name).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+---------+-----+\n",
      "|_c0|   TV|Radio|Newspaper|Sales|\n",
      "+---+-----+-----+---------+-----+\n",
      "| 43|293.6| 27.7|      1.8| 20.7|\n",
      "|129|220.3|   49|      3.2| 24.7|\n",
      "|133|  8.4| 27.2|      2.1|  5.7|\n",
      "|140|184.9| 43.9|      1.7| 20.7|\n",
      "|194|166.8|   42|      3.6| 19.6|\n",
      "+---+-----+-----+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[(df.Radio > 24) & (df.Newspaper < 5)].show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+---------+-----+--------------------+\n",
      "|_c0|   TV|Radio|Newspaper|Sales|             tv_norm|\n",
      "+---+-----+-----+---------+-----+--------------------+\n",
      "|  1|230.1| 37.8|     69.2| 22.1|0.007824268493802813|\n",
      "|  2| 44.5| 39.3|     45.1| 10.4|0.001513167961643...|\n",
      "|  3| 17.2| 45.9|     69.3|  9.3|5.848649200061207E-4|\n",
      "|  4|151.5| 41.3|     58.5| 18.5|0.005151571824472517|\n",
      "|  5|180.8| 10.8|     58.4| 12.9|0.006147882414948061|\n",
      "|  6|  8.7| 48.9|       75|  7.2|2.958328374449564E-4|\n",
      "|  7| 57.5| 32.8|     23.5| 11.8|0.001955217029090...|\n",
      "|  8|120.2| 19.6|     11.6| 13.2|0.004087253685159...|\n",
      "|  9|  8.6|  2.1|        1|  4.8|2.924324600030603...|\n",
      "| 10|199.8|  2.6|     21.2| 10.6| 0.00679395412890831|\n",
      "+---+-----+-----+---------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+---+-----+-----+---------+-----+----+\n",
      "|_c0|   TV|Radio|Newspaper|Sales|cond|\n",
      "+---+-----+-----+---------+-----+----+\n",
      "|  1|230.1| 37.8|     69.2| 22.1|   1|\n",
      "|  2| 44.5| 39.3|     45.1| 10.4|   3|\n",
      "|  3| 17.2| 45.9|     69.3|  9.3|   3|\n",
      "|  4|151.5| 41.3|     58.5| 18.5|   2|\n",
      "|  5|180.8| 10.8|     58.4| 12.9|   1|\n",
      "|  6|  8.7| 48.9|       75|  7.2|   3|\n",
      "|  7| 57.5| 32.8|     23.5| 11.8|   2|\n",
      "|  8|120.2| 19.6|     11.6| 13.2|   1|\n",
      "|  9|  8.6|  2.1|        1|  4.8|   3|\n",
      "| 10|199.8|  2.6|     21.2| 10.6|   1|\n",
      "+---+-----+-----+---------+-----+----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+---+-----+-----+---------+-----+------------------+\n",
      "|_c0|   TV|Radio|Newspaper|Sales|            log_tv|\n",
      "+---+-----+-----+---------+-----+------------------+\n",
      "|  1|230.1| 37.8|     69.2| 22.1|  5.43851399704132|\n",
      "|  2| 44.5| 39.3|     45.1| 10.4|3.7954891891721947|\n",
      "|  3| 17.2| 45.9|     69.3|  9.3|2.8449093838194073|\n",
      "|  4|151.5| 41.3|     58.5| 18.5| 5.020585624949424|\n",
      "|  5|180.8| 10.8|     58.4| 12.9|5.1973914479580765|\n",
      "|  6|  8.7| 48.9|       75|  7.2| 2.163323025660538|\n",
      "|  7| 57.5| 32.8|     23.5| 11.8| 4.051784947803305|\n",
      "|  8|120.2| 19.6|     11.6| 13.2| 4.789157022101107|\n",
      "|  9|  8.6|  2.1|        1|  4.8| 2.151762203259462|\n",
      "| 10|199.8|  2.6|     21.2| 10.6| 5.297316866214453|\n",
      "+---+-----+-----+---------+-----+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('tv_norm', df.TV/df\n",
    "              .groupBy()\n",
    "              .agg(F.sum(\"TV\"))\n",
    "              .collect()[0][0]\n",
    "             ).show(10)\n",
    "\n",
    "df.withColumn('cond', F\n",
    "              .when((df.TV > 100) & (df.Radio < 40), 1)\n",
    "              .when(df.Sales>10, 2)\n",
    "              .otherwise(3)\n",
    "             ).show(10)\n",
    "\n",
    "df.withColumn('log_tv', F.log(df.TV)).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
